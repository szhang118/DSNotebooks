{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to load reviews and movies using lambda functions to parse specific fields in datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_date(r, k):\n",
    "    try:\n",
    "        return datetime.strptime(r[k], \"%d-%b-%Y\") \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_reviews(path, **kwargs):\n",
    "    options = {\"fieldnames\": (\"userid\", \"movieid\", \"rating\", \"timestamp\"), \"delimiter\":\"\\t\"}\n",
    "    options.update(kwargs)\n",
    "    \n",
    "    parse_date = lambda r, k: datetime.datetime.fromtimestamp(float(r[k]))\n",
    "    parse_int = lambda r, k: int(r[k])\n",
    "    \n",
    "    with open(path, \"rb\") as reviews:\n",
    "        reader = csv.DictReader(reviews, **options)\n",
    "        for row in reader:\n",
    "            row[\"userid\"] = parse_int(row, \"userid\")\n",
    "            row[\"movieid\"] = parse_int(row, \"movieid\")\n",
    "            row[\"rating\"] = parse_int(row, \"rating\")\n",
    "            row[\"timestamp\"] = parse_date(row, \"timestamp\")\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_movies(path, **kwargs):\n",
    "    options = {\"fieldnames\":(\"movieid\", \"title\", \"release\", \"video\", \"url\"), \"delimiter\":\"|\", \"restkey\":\"genre\"}\n",
    "    options.update(kwargs)\n",
    "    \n",
    "    parse_int = lambda r, k: int(r[k])\n",
    "    \n",
    "    with open(path, \"rb\") as movies:\n",
    "        reader = csv.DictReader(movies, **options)\n",
    "        for row in reader:\n",
    "            row[\"movieid\"] = parse_int(row, \"movieid\")\n",
    "            row[\"release\"] = parse_date(row, \"release\")\n",
    "            row[\"video\"] = parse_date(row, \"video\")\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a MovieLens class that will be used as a data structure to build the recommender model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from operator import itemgetter\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MovieLens(object):\n",
    "    \"\"\"Date structure for recommendation model.\"\"\"\n",
    "    \n",
    "    def __init__(self, udata, uitem):\n",
    "        self.udata = udata\n",
    "        self.uitem = uitem\n",
    "        self.movies = {}\n",
    "        self.reviews = defaultdict(dict)\n",
    "        self.load_dataset()\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        for movie in load_movies(self.uitem):\n",
    "            self.movies[movie[\"movieid\"]] = movie\n",
    "            \n",
    "        for review in load_reviews(self.udata):\n",
    "            self.reviews[review[\"userid\"]][review[\"movieid\"]] = review\n",
    "    \n",
    "    def reviews_for_movie(self, movieid):\n",
    "        \"\"\"Yields reviews for a given movie.\"\"\"\n",
    "        for review in self.reviews.values():\n",
    "            if movieid in review:\n",
    "                yield review[movieid]\n",
    "    \n",
    "    def average_reviews(self):\n",
    "        \"\"\"Average star ratings for all movies.\"\"\"\n",
    "        for movieid in self.movies:\n",
    "            reviews = list(r[\"rating\"] for r in self.reviews_for_movie(movieid))\n",
    "            average = sum(reviews) / float(len(reviews))\n",
    "            yield (movieid, average, len(reviews))\n",
    "    \n",
    "    def bayesian_average(self, c=59, m=3):\n",
    "        \"\"\"Reports Bayesian average to penalize movies with small number of reviews.\"\"\"\n",
    "        for movieid in self.movies:\n",
    "            reviews = list(r[\"rating\"] for r in self.reviews_for_movie(movieid))\n",
    "            average = ((c*m) + sum(reviews)) / float(c + len(reviews))\n",
    "            yield(movieid, average, len(reviews))\n",
    "        \n",
    "    def top_rated(self, n = 10, C=59, M=3):\n",
    "        \"\"\"Yields top n rated movies.\"\"\"\n",
    "        return heapq.nlargest(n, self.bayesian_average(c=C, m=M), key=itemgetter(1))\n",
    "    \n",
    "    def shared_preferences(self, criticA, criticB):\n",
    "        \"\"\"Returns intersection of ratings for 2 critics.\"\"\"\n",
    "        if criticA not in self.reviews:\n",
    "            raise KeyError(\"Couldn't find critic %s in data\" % criticA)\n",
    "        if criticB not in self.reviews:\n",
    "            raise KeyError(\"Couldn't find critic %s in data\" % criticB)\n",
    "        \n",
    "        moviesA = set(self.reviews[criticA].keys())\n",
    "        moviesB = set(self.reviews[criticB].keys())\n",
    "        shared = moviesA & moviesB\n",
    "        \n",
    "        #create a shared reviews dictionary to return\n",
    "        reviews = {}\n",
    "        for movieid in shared:\n",
    "            reviews[movieid] = (self.reviews[criticA][movieid][\"rating\"], self.reviews[criticB][movieid][\"rating\"])\n",
    "        return reviews\n",
    "    \n",
    "    def shared_critics(self, movieA, movieB):\n",
    "        \"\"\"Returns intersection of critics for A and B.\"\"\"\n",
    "        if movieA not in self.movies:\n",
    "            raise KeyError(\"Couldn't find movie '%s' in data.\" % movieA)\n",
    "        if movieB not in self.movies:\n",
    "            raise KeyError(\"Couldn't find movie '%s' in data.\" % movieB)\n",
    "        \n",
    "        criticsA = set(critic for critic in self.reviews if movieA in self.reviews[critic])\n",
    "        criticsB = set(critic for critic in self.reviews if movieB in self.reviews[critic])\n",
    "        shared = criticsA & criticsB\n",
    "        \n",
    "        reviews = {}\n",
    "        for critic in shared:\n",
    "            reviews[critic] = (self.reviews[critic][movieA][\"rating\"], self.reviews[critic][movieB][\"rating\"])\n",
    "        return reviews\n",
    "    \n",
    "    def euclidean_distance(self, criticA, criticB):\n",
    "        \"\"\"Report Euclidean distance of two critics on shared preference vectors.\"\"\"\n",
    "        preferences = self.shared_preferences(criticA, criticB)\n",
    "        if len(preferences) == 0: return 0 #if no movies rated in common\n",
    "        sumSquares = sum([pow(a -b, 2) for a, b in preferences.values()])\n",
    "        return 1.0/(1.0 + sqrt(sumSquares))\n",
    "    \n",
    "    def manhattan_distance(self, criticA, criticB):\n",
    "        \"\"\"Report Manhattan distance of two critics on shared preference vectors.\"\"\"\n",
    "        preferences = self.shared_preferences(criticA, criticB)\n",
    "        if len(preferences) == 0: return 0\n",
    "        manhattan = sum([abs(a - b) for a, b in preferences.values()])\n",
    "        return 1.0 / (1.0 + manhattan)\n",
    "    \n",
    "    def pearson_correlation(self, criticA, criticB):\n",
    "        \"\"\"Return Pearson correlation with 2 critics on shared vectors.\"\"\"\n",
    "        preferences = self.shared_preferences(criticA, criticB)\n",
    "        length = len(preferences)\n",
    "        if length == 0: return 0\n",
    "        sumA = sumB = sumSquareA = sumSquareB = sumProducts = 0\n",
    "        for a, b in preferences.values():\n",
    "            sumA += a\n",
    "            sumB += b\n",
    "            sumSquareA += pow(a, 2)\n",
    "            sumSquareB += pow(b, 2)\n",
    "            sumProducts += a * b\n",
    "        numerator = (sumProducts*length) - sumA * sumB\n",
    "        denominator = sqrt(((sumSquareA*length) - pow(sumA, 2))*((sumSquareB*length) - pow(sumB, 2)))\n",
    "        if denominator == 0: return 0\n",
    "        return abs(numerator / denominator)\n",
    "    \n",
    "    def similar_critics(self, user, metric=\"euclidean\", n=None):\n",
    "        \"\"\"Finds and ranks similar critics for the user according to the specified distance metric.\"\"\"\n",
    "        metrics = {\"euclidean\":self.euclidean_distance, \"pearson\": self.pearson_correlation, \"manhattan\":self.manhattan_distance}\n",
    "        distance = metrics.get(metric, None)\n",
    "        \n",
    "        #error handling\n",
    "        if user not in self.reviews:\n",
    "            raise KeyError(\"Unknown user, '%s'.\" % user)\n",
    "        if not distance or not callable(distance):\n",
    "            raise KeyError(\"Unknown distance metric %s.\" % metric)\n",
    "        critics = {}\n",
    "        for critic in self.reviews:\n",
    "            if critic == user:\n",
    "                continue\n",
    "            critics[critic] = distance(user, critic)\n",
    "        if n: return heapq.nlargest(n, critics.items(), key=itemgetter(1))\n",
    "        return critics\n",
    "    \n",
    "    def similar_items(self, movie, metric=\"euclidean\", n = None):\n",
    "        metrics = {\"euclidean\":self.euclidean_distance, \"pearson\": self.pearson_correlation, \"manhattan\":self.manhattan_distance}\n",
    "        distance = metrics.get(metric, None)\n",
    "        \n",
    "        if movie not in self.reviews:\n",
    "            raise KeyError(\"Unknown movie, '%s'.\" % movie)\n",
    "        if not distance or not callable(distance):\n",
    "            raise KeyError(\"Unknown distance metric '%s'.\" % metric)\n",
    "        \n",
    "        items = {}\n",
    "        for item in self.movies:\n",
    "            if item == movie: continue\n",
    "            items[item] = distance(item, movie)\n",
    "        if n: return heapq.nlargest(n, items.items(), key= itemgetter(1))\n",
    "        return items\n",
    "    \n",
    "    def predict_ranking(self, user, movie, metric=\"euclidean\", critics=None):\n",
    "        \"\"\"Predicts ranking user might give a movie based on weighted verage of similar critics.\"\"\"\n",
    "        critics = critics or self.similar_critics(user, metric=metric)\n",
    "        total = 0.0\n",
    "        simsum = 0.0\n",
    "        for critic, similarity in critics.items():\n",
    "            if movie in self.reviews[critic]:\n",
    "                total += similarity*self.reviews[critic][movie][\"rating\"]\n",
    "                simsum += similarity\n",
    "        if simsum == 0.0: return 0.0\n",
    "        return total / simsum\n",
    "    \n",
    "    def predict_all_rankings(self, user, metric=\"euclidean\", n = None):\n",
    "        \"\"\"Predicts all rankings for all movies, if n specified return top n with predictions.\"\"\"\n",
    "        critics = self.similar_critics(user, metric=metric)\n",
    "        movies = {movie:self.predict_ranking(user, movie, metric, critics) for movie in self.movies}\n",
    "        if n: return heapq.nlargest(n, movies.items(), key=itemgetter(1))\n",
    "        return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"u.data\"\n",
    "item = \"u.item\"\n",
    "model = MovieLens(data, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.394 average rating (298 reviews)] Schindler's List (1993)\n",
      "[4.371 average rating (283 reviews)] Shawshank Redemption, The (1994)\n",
      "[4.370 average rating (243 reviews)] Casablanca (1942)\n",
      "[4.324 average rating (583 reviews)] Star Wars (1977)\n",
      "[4.316 average rating (112 reviews)] Close Shave, A (1995)\n",
      "[4.312 average rating (267 reviews)] Usual Suspects, The (1995)\n",
      "[4.302 average rating (118 reviews)] Wrong Trousers, The (1993)\n",
      "[4.296 average rating (209 reviews)] Rear Window (1954)\n",
      "[4.244 average rating (390 reviews)] Silence of the Lambs, The (1991)\n",
      "[4.240 average rating (413 reviews)] Godfather, The (1972)\n"
     ]
    }
   ],
   "source": [
    "for mid, avg, num in model.top_rated(10, C=25, M=3.52986):\n",
    "    title = model.movies[mid][\"title\"]\n",
    "    print \"[%0.3f average rating (%i reviews)] %s\" %(avg, num, title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of reviews per movie: 59.4530321046\n"
     ]
    }
   ],
   "source": [
    "print \"Average number of reviews per movie:\", float(sum(num for mid, avg, num in model.average_reviews())) / len(model.movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reviw rating: 3.52986\n"
     ]
    }
   ],
   "source": [
    "print \"Average reviw rating:\", float(sum(avg*num for mid, avg, num in model.average_reviews())) \\\n",
    "/float(sum(num for mid, avg, num in model.average_reviews()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.102302162992\n"
     ]
    }
   ],
   "source": [
    "print model.euclidean_distance(232, 532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0227272727273\n"
     ]
    }
   ],
   "source": [
    "print model.manhattan_distance(232, 532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0602579353839\n"
     ]
    }
   ],
   "source": [
    "print model.pearson_correlation(232, 532)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most similar users for user 232."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 688: 1.0000\n",
      " 914: 1.0000\n",
      "  47: 0.5000\n",
      "  78: 0.5000\n",
      " 170: 0.5000\n",
      " 335: 0.5000\n",
      " 341: 0.5000\n",
      " 101: 0.4142\n",
      " 155: 0.4142\n",
      " 309: 0.4142\n"
     ]
    }
   ],
   "source": [
    "for item in model.similar_critics(232, \"euclidean\", n = 10):\n",
    "    print \"%4i: %0.4f\" % item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  33: 1.0000\n",
      "  36: 1.0000\n",
      " 155: 1.0000\n",
      " 260: 1.0000\n",
      " 289: 1.0000\n",
      " 302: 1.0000\n",
      " 309: 1.0000\n",
      " 317: 1.0000\n",
      " 511: 1.0000\n",
      " 769: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for item in model.similar_critics(232, \"pearson\", n = 10):\n",
    "    print \"%4i: %0.4f\" % item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35413151722\n",
      "4.3566797826\n"
     ]
    }
   ],
   "source": [
    "print model.predict_ranking(422, 50)\n",
    "print model.predict_ranking(422, 50, \"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0000: Prefontaine (1997)\n",
      "5.0000: Santa with Muscles (1996)\n",
      "5.0000: Marlene Dietrich: Shadow and Light (1996) \n",
      "5.0000: Star Kid (1997)\n",
      "5.0000: Aiqing wansui (1994)\n",
      "5.0000: Someone Else's America (1995)\n",
      "5.0000: Great Day in Harlem, A (1994)\n",
      "5.0000: Saint of Fort Washington, The (1993)\n",
      "4.9539: Anna (1996)\n",
      "4.8175: Innocents, The (1961)\n"
     ]
    }
   ],
   "source": [
    "for mid, rating in model.predict_all_rankings(578, \"pearson\", 10):\n",
    "    print \"%0.4f: %s\" % (rating, model.movies[mid][\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Recommender(object):\n",
    "\n",
    "    @classmethod\n",
    "    def load(klass, pickle_path):\n",
    "        \"\"\"\n",
    "        Instantiates the class by deserializing the pickle. Note that the\n",
    "        object returned may not be an exact match to the code in this\n",
    "        class (if it was saved before updates).\n",
    "        \"\"\"\n",
    "        with open(pickle_path, 'rb') as pkl:\n",
    "            return pickle.load(pkl)\n",
    "\n",
    "    def __init__(self, udata, description=None):\n",
    "        self.udata   = udata\n",
    "        self.users   = None\n",
    "        self.movies  = None\n",
    "        self.reviews = None\n",
    "\n",
    "        # Descriptive properties\n",
    "        self.build_start  = None\n",
    "        self.build_finish = None\n",
    "        self.description  = None\n",
    "\n",
    "        # Model properties\n",
    "        self.model        = None\n",
    "        self.features     = 2\n",
    "        self.steps        = 5000\n",
    "        self.alpha        = 0.0002\n",
    "        self.beta         = 0.02\n",
    "\n",
    "        self.load_dataset()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads an index of users and movies as a heap and a reviews table\n",
    "        as a N x M array where N is the number of users and M is the number\n",
    "        of movies. Note that order matters so that we can look up values\n",
    "        outside of the matrix!\n",
    "        \"\"\"\n",
    "        self.users  = set([])\n",
    "        self.movies = set([])\n",
    "        for review in load_reviews(self.udata):\n",
    "            self.users.add(review['userid'])\n",
    "            self.movies.add(review['movieid'])\n",
    "\n",
    "        self.users  = sorted(self.users)\n",
    "        self.movies = sorted(self.movies)\n",
    "\n",
    "        self.reviews = np.zeros(shape=(len(self.users), len(self.movies)))\n",
    "        for review in load_reviews(self.udata):\n",
    "            uid = self.users.index(review['userid'])\n",
    "            mid = self.movies.index(review['movieid'])\n",
    "            self.reviews[uid, mid] = review['rating']\n",
    "\n",
    "    def build(self, output=None, alternate=False):\n",
    "        \"\"\"\n",
    "        Trains the model by employing matrix factorization on our training\n",
    "        data set, the sparse reviews matrix. The model is the dot product\n",
    "        of the P and Q decomposed matrices from the factorization.\n",
    "        \"\"\"\n",
    "        options = {\n",
    "            'K':     self.features,\n",
    "            'steps': self.steps,\n",
    "            'alpha': self.alpha,\n",
    "            'beta':  self.beta,\n",
    "        }\n",
    "\n",
    "        self.build_start = time.time()\n",
    "        nnmf = factor2 if alternate else factor\n",
    "        self.P, self.Q = nnmf(self.reviews, **options)\n",
    "        self.model = np.dot(self.P, self.Q.T)\n",
    "        self.build_finish = time.time()\n",
    "\n",
    "        if output:\n",
    "            self.dump(output)\n",
    "\n",
    "    def dump(self, pickle_path):\n",
    "        \"\"\"\n",
    "        Dump the object into a serialized file using the pickle module.\n",
    "        This will allow us to quickly reload our model in the future.\n",
    "        \"\"\"\n",
    "        with open(pickle_path, 'wb') as pkl:\n",
    "            pickle.dump(self, pkl)\n",
    "\n",
    "    def sparsity(self):\n",
    "        \"\"\"\n",
    "        Report the percent of elements that are zero in the array\n",
    "        \"\"\"\n",
    "        return 1 - self.density()\n",
    "\n",
    "    def density(self):\n",
    "        \"\"\"\n",
    "        Return the percent of elements that are nonzero in the array\n",
    "        \"\"\"\n",
    "        nonzero = float(np.count_nonzero(self.reviews))\n",
    "        return nonzero / self.reviews.size\n",
    "\n",
    "    def error_rate(self):\n",
    "        \"\"\"\n",
    "        Compute the sum squared error of the trained model.\n",
    "        \"\"\"\n",
    "        error = 0.0\n",
    "        rows, cols = self.reviews.shape\n",
    "        for idx in xrange(rows):\n",
    "            for jdx in xrange(cols):\n",
    "                if self.reviews[idx, jdx] > 0:\n",
    "                    error += (self.model[idx, jdx] - self.reviews[idx, jdx]) ** 2\n",
    "                    print error\n",
    "        return error\n",
    "\n",
    "    def predict_ranking(self, user, movie):\n",
    "        uidx = self.users.index(user)\n",
    "        midx = self.movies.index(movie)\n",
    "        return self.model[uidx, midx]\n",
    "\n",
    "    def top_rated(self, user, n=12):\n",
    "        movies = [(mid, self.predict_ranking(user, mid)) for mid in self.movies]\n",
    "        return heapq.nlargest(n, movies, key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Recommender(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sparsity: 0.936953306358\n",
      "Model density: 0.0630466936422\n"
     ]
    }
   ],
   "source": [
    "print \"Model sparsity:\", model.sparsity()\n",
    "print \"Model density:\", model.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(R, K):\n",
    "    N, M = R.shape\n",
    "    P = np.random.randn(N, K)\n",
    "    Q = np.random.randn(M, K)\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factor(R, P=None, Q=None, K=2, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    \"\"\"\n",
    "    Performs matrix factorization on R with given parameters.\n",
    "    :param R: A matrix to be factorized, dimension N x M\n",
    "    :param P: an initial matrix of dimension N x K\n",
    "    :param Q: an initial matrix of dimension M x K\n",
    "    :param K: the number of latent features\n",
    "    :param steps: the maximum number of iterations to optimize in\n",
    "    :param alpha: the learning rate for gradient descent\n",
    "    :param beta:  the regularization parameter\n",
    "    :returns: final matrices P and Q\n",
    "    \"\"\"\n",
    "\n",
    "    if not P or not Q:\n",
    "        P, Q = initialize(R, K)\n",
    "    Q = Q.T\n",
    "\n",
    "    rows, cols = R.shape\n",
    "    for step in xrange(steps):\n",
    "        for i in xrange(rows):\n",
    "            for j in xrange(cols):\n",
    "                if R[i,j] > 0:\n",
    "                    eij = R[i,j] - np.dot(P[i,:], Q[:,j])\n",
    "                    for k in xrange(K):\n",
    "                        P[i,k] = P[i,k] + alpha * (2 * eij * Q[k,j] - beta * P[i,k])\n",
    "                        Q[k,j] = Q[k,j] + alpha * (2 * eij * P[i,k] - beta * Q[k,j])\n",
    "\n",
    "        e  = 0\n",
    "        for i in xrange(rows):\n",
    "            for j in xrange(cols):\n",
    "                if R[i,j] > 0:\n",
    "                    e = e + pow(R[i,j] - np.dot(P[i,:], Q[:,j]), 2)\n",
    "                    for k in xrange(K):\n",
    "                        e = e + (beta/2) * (pow(P[i,k], 2) + pow(Q[k,j], 2))\n",
    "        if e < 0.001:\n",
    "            break\n",
    "\n",
    "    return P, Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
